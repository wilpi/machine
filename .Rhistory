#To check the new N?? of observations
dim(myTraining)
myTraining <- myTraining[c(-1)]
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(modFitA1)
library(rattle)
fancyRpartPlot(modFitA1)
```{r plot, fig.height=4 }
fancyRpartPlot(modFitA1)
```
```{r result="hide"}
fancy<-fancyRpartPlot(modFitA1)
dev.copy(png, file = "plot1.png")
dev.off()
```
```{r result="hide"}
fancy<-fancyRpartPlot(modFitA1)
dev.copy(png, file = "plot1.png")
dev.off()
```
---
title: "Assignament Maching Learning"
author: "WIlmer Fuentes Neira"
date: "Sunday, February 22, 2015"
output: html_document
---
markdownToHTML("final_job.md", "final_job.html")
```{r}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
```
```{r}
options(warn=-1)
library(caret)
library(randomForest)
library(Hmisc)
library(foreach)
library(doParallel)
set.seed(12345)
```
```{r, echo=F, result="hide"}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```
```{r}
#summary(training)
#describe(training)
#sapply(data, class)
#str(data)
```
```
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
#To check the new N?? of observations
dim(myTraining)
```
```{r}
myTraining <- myTraining[c(-1)]
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
```
```{r result="hide"}
fancy<-fancyRpartPlot(modFitA1)
dev.copy(png, file = "plot1.png")
dev.off()
```
![plot of chunk unnamed-chunk-2](plot1.png)
knit("final_job.Rmd")
markdownToHTML("Foo.md", "Foo.html")
\end{document}
knit("final_job.Rmd")
knitr("final_job.Rmd")
library(knit)
library(knitr)
knitr("final_job.Rmd")
knit("final_job.Rmd")
knitr("final_job.Rmd")
knit("final_job.Rmd")
knit("final_job.Rmd")
knit("final_job.Rmd")
setwd("F:/Documents/GitHub/machine")
library(knitr)
classe
mytesting$classe
dim(classe)
dim(mytesting$classe)
dim(myTesting$classe)
dim(myTesting$classe)
modFitB1 <- randomForest(classe ~. , data=myTraining)
dim(myTraining$classe)
confusionMatrix(predictionsB1, myTesting$classe)
confusionMatrix(predictionsA1, myTesting$classe)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
confusionMatrix(predictionsA1, myTesting$classe)
modFitB1 <- randomForest(classe ~. , data=myTraining)
modFitB1 <- randomForest(classe~. , data=myTraining)
modFitB1 <- randomForest(factor(classe) ~. , data=myTraining)
modFitB1 <- randomForest(factor(classe) ~  ., data=myTraining[,cols])
modFitB1 <- randomForest(factor(classe) ~  ., data=myTraining)
library(randomForest)
modFitB1 <- randomForest(classe~. , data=myTraining)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
class(myTraining$classe)
summary(myTraining$classe)
summary(myTesting$classe)
modFitB1 <- randomForest(classe ~. , data=myTraining)
modFitB1 <- randomForest(myTraining$classe ~. , data=myTraining)
modFitB1 <- randomForest(myTraining$classe ~ .,)
modFitB1 <- randomForest(myTraining$classe ~ ., data=myTraining)
dim(myTraining)
table(myTraining$classe)
summary(myTraining$classe)
modFit2 <- randomForest(classe ~. , data=myTraining)
modFit2 <- randomForest(classe ~. , data=myTraining, na.action = na.omit)
```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining, , na.action = na.omit)
```
modFitB1 <- randomForest(classe ~. , data=myTraining, , na.action = na.omit)
confusionMatrix(predictionsB1, myTesting$classe)
confusionMatrix(predictionsB1, myTesting$classe)
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining,na.action = na.omit)
```{r}
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
confusionMatrix(predictionsB1, myTesting$classe)
predictionsB2 <- predict(modFitB1, testing, type = "class")
dimm(testing)
dim(testing)
dim(training)
dim(testing)
predictionsB2 <- predict(modFitB1, testing, type = "class")
predictionsB2 <- predict(modFitB1, testing)
sapply(training, class)
sapply(testing, class)
sapply(myTesting, class)
sapply(myTraining, class)
predictionsB2 <- predict(modFitB1, testing, type = "class")
sapply(testing, class)
sapply(training, class)
predictionsB2 <- predict(modFitB1, testing, type = "class")
dim(testing)
predictionsB2 <- predict(modFitB1, testing, type = "class", na.action = na.omit)
predictionsB2 <- predict(modFitB1, testing, type = "class", na.action=na.omit)
modFitB1
dim(testing)
dim(Mytesting)
dim(myTesting)
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
dim(testing)
predictionsB2 <- predict(modFitB1, testing, type = "class")
#for(i in 1:n){
dim(testing)
```
dim(testing)
---
title: "Assignament Maching Learning"
author: "WIlmer Fuentes Neira"
date: "Sunday, February 22, 2015"
output:
html_document:
fig_height: 6
fig_width: 9
---
markdownToHTML("final_job.md", "final_job.html")
```{r}
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
```
```{r}
options(warn=-1)
library(caret)
library(randomForest)
library(Hmisc)
library(foreach)
library(doParallel)
set.seed(12345)
```
```{r, echo=F, result="hide"}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```
```{r,echo=F, result="hide"}
#summary(training)
#describe(training)
#sapply(data, class)
#sapply(training, class)
#sapply(testing, class)
#str(data)
```
```
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
#To check the new N?? of observations
dim(myTraining)
```
Transformation 2: Killing first column of Dataset - ID Removing first ID variable so that it does not interfer with ML Algorithms:
```{r}
myTraining <- myTraining[c(-1)]
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
```
Transformation 3: Cleaning Variables with too many NAs. For Variables that have more than a 60% threshold of NA’s I’m going to leave them out:
```{r}
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
for(j in 1:length(trainingV3)) {
if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
trainingV3 <- trainingV3[ , -j] #Remove that column
}
}
}
}
```
```{r}
#Seting back to our set:
myTraining <- trainingV3
rm(trainingV3)
```
Now let us do the exact same 3 transformations but for our myTesting and testing data sets.
```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
#To check the new N?? of observations
dim(myTesting)
```
```{r}
dim(testing)
```
```{r}
for (i in 1:length(testing) ) {
for(j in 1:length(myTraining)) {
if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
class(testing[j]) <- class(myTraining[i])
}
}
}
#And to make sure Coertion really worked, simple smart ass technique:
testing <- rbind(myTraining[2, -58] , testing) #note row 2 does not mean anything, this will be removed right.. now:
testing <- testing[-1,]
```
```{r }
fancy<-fancyRpartPlot(modFitA1)
```
Predicting:
```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class", na.action = na.omit)
```
```{r}
confusionMatrix(predictionsA1, myTesting$classe)
```
##Using ML algorithms for prediction: Random Forests
```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining, na.action = na.omit)
```
Predicting in-sample error:
```{r}
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
```
(Moment of truth) Using confusion Matrix to test results:
```{r}
confusionMatrix(predictionsB1, myTesting$classe)
```
Random Forests yielded better Results, as expected!
## Generating Files to submit as answers for the Assignment:
Finally, using the provided Test Set out-of-sample error.
For Random Forests we use the following formula, which yielded a much better prediction in in-sample:
```{r}
#predictionsB2 <- predict(modFitB1, testing, type = "class")
```
```{r}
#pml_write_files = function(x){
#  n = length(x)
#for(i in 1:n){
#   filename = paste0("problem_id_",i,".txt")
#    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#  }
#}
#pml_write_files(predictionsB2)
```
```
Error in eval(expr, envir, enclos) : object 'max_roll_belt' not found
```{r}
predictionsB2 <- predict(modFitB1, testing, type = "class")
```{r}
predictionsB2 <- predict(modFitB1, testing, type = "class")
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(predictionsB2)
knitr("final_job.Rmd")
knit("final_job.Rmd")
knit("final_job.Rmd")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
library(ggplot2)
library(rattle)
training<-segmentationOriginal[segmentationOriginal$Case=="Train",]
testing<-segmentationOriginal[segmentationOriginal$Case=="Test",]
set.seed(125)
model<-train(Class ~ .,
data = training,
method = "rpart")
fancyRpartPlot(model$finalModel)
summary(model$finalModel)
predData[1,c(103,50,85)]=c(23000,10,2)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
predData[1,c(103,50,85)]=c(23000,10,2)
predData
edit(predData)
str(predData)
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predData[4,c(103,50,85)]=c(8,100,2)
predict(fit,predData)
predData[]
predData[1,]
predData[1,103]
predData[1,50]
predData[1,85]
predData[2,103]
predData[4,103]
predData[,103]
predict(fit,predData)
library(pgmm)
data(olive)
install.packages("pgmm")
data(olive)
olive = olive[,-1]
data(olive)
olive = olive[,-1]
str(olive)
library(pgmm)
data(olive)
str(olive)
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
pred <- predict(fit,newdata)
fancyRpartPlot(fit$finalModel)
summary(fit)
summary(pred )
predict(model, newdata)
predict(fit, newdata)
pred
str(olive)
str(pred)
str(newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
#SAheart<-SAheart[c("age", "alcohol", "obesity", "tobacco", "typea", "ldl", "chd")]
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,data=trainSA,method="glm",family="binomial")
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(model, trainSA))
missClass(testSA$chd, predict(model, testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
str(vowel)
fit <- randomForest(y~.,data=vowel.train,importance=TRUE)
imps <- varImp(fit)
order(imps)
imps
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
imps
par(mfrow=c(1,2))
##Reporte respuestas Ejercicio 1
slices <- table(bd2$type)
lbls <- c("Mixed methods", "Qualitative","Quantitative")
pct <- round(slices/length(bd2$type)*100,digits=1)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Lo que encontramos")
legend("topleft", paste("Total de articulos",length(bd2$type)),bty="n")
##Reporte respuestas Evaluadores
vota3<-getURL("https://docs.google.com/spreadsheets/d/1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw/export?format=csv&id=1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw&gid=1596733197")
bd3<-read.csv(textConnection(vota3))
slices2 <- sapply(bd3[,2:4], mean, na.rm=TRUE)
lbls <- c("Mixed methods", "Qualitative","Quantitative")
pct <- round(slices2,digits=1)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices2,labels = lbls, col=rainbow(length(lbls)),
main="Lo que creemos")
legend("topleft", paste("Esto creemos",length(bd3$Mixto)),bty="n")
slicessapply(bd3[,2:4], mean, na.rm=TRUE)
#SETEANDO PROCEDIMIENTOS
require(RCurl)
options(RCurlOptions = list(capath = system.file("CurlSSL", "cacert.pem",
package = "RCurl"), ssl.verifypeer = FALSE))
vota<-getURL("https://docs.google.com/spreadsheets/d/1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw/export?format=csv&id=1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw&gid=1891395865")
bd1<-read.csv(textConnection(vota))
edit(bd1)
attach(bd1)
library(XLConnect)
bd2 <- readWorksheet(loadWorkbook("f:/drive/paperUCH.xlsx"),sheet=1)
table(bd2$type)/length(bd2$type)*100
bd1$puntaje<-bd1[,4]+bd1[,5]+bd1[,6]+bd1[,7]+bd1[,8]+bd1[,9]+bd1[,10]
x<-merge(x = bd1, y = bd2, by = "IDpaper", all.x=TRUE)
m<-tapply(x$puntaje, x$title,mean)
m<-sort(m, decreasing = TRUE)
ju<-t(t(m))
mu<-as.data.frame(ju)
edit(ju)
edit(bd1)
par(mfrow=c(1,2))
##Reporte respuestas Ejercicio 1
slices <- table(bd2$type)
lbls <- c("Mixed methods", "Qualitative","Quantitative")
pct <- round(slices/length(bd2$type)*100,digits=1)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices,labels = lbls, col=rainbow(length(lbls)),
main="Lo que encontramos")
legend("topleft", paste("Total de articulos",length(bd2$type)),bty="n")
##Reporte respuestas Evaluadores
vota3<-getURL("https://docs.google.com/spreadsheets/d/1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw/export?format=csv&id=1WDVO3NPU2ZD-ruTQ6-E6a3UC6EJ5Y2hLz2Et-0qPlgw&gid=1596733197")
bd3<-read.csv(textConnection(vota3))
slices2 <- sapply(bd3[,2:4], mean, na.rm=TRUE)
lbls <- c("Mixed methods", "Qualitative","Quantitative")
pct <- round(slices2,digits=1)
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % to labels
pie(slices2,labels = lbls, col=rainbow(length(lbls)),
main="Lo que creemos")
legend("topleft", paste("Esto creemos",length(bd3$Mixto)),bty="n")
slicessapply(bd3[,2:4], mean, na.rm=TRUE)
